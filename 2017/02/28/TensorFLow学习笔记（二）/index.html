<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>TensorFLow学习笔记（二） | Htxf-fxtH</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="官方文档第二篇。MNIST for ML Beginners先介绍MNIST，还有softmax regression（softmax回归），然后利用tensorflow构建一个模型。
1. MNISTMNIST对于机器学习就相当于Hello World对于一门编程语言。
MNIST是一个简单的计算机视觉数据集。有好多手写数字图片，并有相应的label标记，说明这张图片是数字几（0~9）。
一个简">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFLow学习笔记（二）">
<meta property="og:url" content="http://yoursite.com/2017/02/28/TensorFLow学习笔记（二）/index.html">
<meta property="og:site_name" content="Htxf-fxtH">
<meta property="og:description" content="官方文档第二篇。MNIST for ML Beginners先介绍MNIST，还有softmax regression（softmax回归），然后利用tensorflow构建一个模型。
1. MNISTMNIST对于机器学习就相当于Hello World对于一门编程语言。
MNIST是一个简单的计算机视觉数据集。有好多手写数字图片，并有相应的label标记，说明这张图片是数字几（0~9）。
一个简">
<meta property="og:image" content="http://yoursite.com/..\TensorFLow学习笔记（二）\tensor.jpg">
<meta property="og:updated_time" content="2017-03-01T05:43:09.710Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFLow学习笔记（二）">
<meta name="twitter:description" content="官方文档第二篇。MNIST for ML Beginners先介绍MNIST，还有softmax regression（softmax回归），然后利用tensorflow构建一个模型。
1. MNISTMNIST对于机器学习就相当于Hello World对于一门编程语言。
MNIST是一个简单的计算机视觉数据集。有好多手写数字图片，并有相应的label标记，说明这张图片是数字几（0~9）。
一个简">
<meta name="twitter:image" content="http://yoursite.com/..\TensorFLow学习笔记（二）\tensor.jpg">
  
    <link rel="alternative" href="/atom.xml" title="Htxf-fxtH" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Open+Sans|">
  <link href='https://fonts.googleapis.com/css?family=Dancing+Script' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/style.css">
  

  <link rel="stylesheet" href="/css/base.css">
  <link rel="stylesheet" href="/css/home.css">
  <link rel="stylesheet" href="/css/animate.css">
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script type="text/javascript" src="http://code.jQuery.com/jquery-latest.js"></script>
  <script src="/js/base.js"></script>
  <script src="/js/jquery.cookie.js"></script>
  <script src="/fancybox/jquery.fancybox.js"></script>
  <script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>



<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
    <div id="insert" class="home_logo"></div>
    <div class="hamburger animated shake">
      <i ></i>
      <ul>
        <li class="home"><a href="/">Home<span></span></a></li>
        <li><a href="http://cpbxx.lofter.com/" target="blank">Works<span></span></a></li>
        <li class="home_notes_showon" ><a href="/">Notes<span></span></a></li>
        <li><a href="./tucaos.html">Tucaos<span></span></a></li>
      </ul>
    </div>
</header>

      <div class="outer">
        <section id="main"><div class="main_center"><article id="post-TensorFLow学习笔记（二）" class="article article-type-post" itemscope itemprop="blogPost">

  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      TensorFLow学习笔记（二）
    </h1>
  


        <div class="article-meta">
          <span class="article-date">
  <time datetime="2017-02-28T09:09:08.000Z" itemprop="datePublished">Feb 28 2017</time>
</span>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>官方文档第二篇。<a href="https://www.tensorflow.org/get_started/mnist/beginners" target="_blank" rel="external">MNIST for ML Beginners</a><br>先介绍MNIST，还有softmax regression（softmax回归），然后利用tensorflow构建一个模型。</p>
<h3 id="1-MNIST"><a href="#1-MNIST" class="headerlink" title="1. MNIST"></a>1. MNIST</h3><p>MNIST对于机器学习就相当于Hello World对于一门编程语言。</p>
<p>MNIST是一个简单的计算机视觉数据集。有好多手写数字图片，并有相应的<strong>label标记</strong>，说明这张图片是数字几（0~9）。</p>
<p>一个简单的任务就是，训练一个模型，然后给它一张图片，它会预测出这张图片是数字几。（是一个分类Classification问题，多分类）</p>
<a id="more"></a>
<p>MNIST是放在<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="external">Yann LeCun’s website</a> (CNN的提出者)，Google提供代码可以将数据下载下来，并已经把数据分为了<strong>training data</strong>训练数据、<strong>validation data</strong>验证数据、<strong>test data</strong>测试数据。训练数据是训练模型用，验证数据也是在训练模型的过程中用（比如防止模型对于训练数据的过拟合，模型对于训练数据的表现在越来越好，而对于验证数据的表现并没有越来越好），测试数据是为了验证模型的泛化能力，在训练过程中是不能被模型“看到的“。这些数据是很多的。</p>
<p>具体到计算机上的数据表示。图片本身是28像素 X 28像素的，用一个28 X 28的数字矩阵表示。图片上越黑，对应数字矩阵的数字越大（图片都是灰度的）。然后将这个数字矩阵“压扁”成为一个28 * 28 = 784维的向量。用x表示一张图片，那么x = [0.1, 0.2, 0., 0.5……]，一共784个值。对于这张图片的标签，用y表示，它是一个“<strong>one-hot vector</strong>”，假设这张图片是数字3，那y就是[0, 0, 1, 0, 0, 0, 0, 0, 0]。</p>
<h3 id="2-Softmax-Regressions"><a href="#2-Softmax-Regressions" class="headerlink" title="2. Softmax Regressions"></a>2. Softmax Regressions</h3><p>Softmax回归。机器学习的问题分为两种，一种分类Classification问题，一种回归Regression问题。分类问题的输出是离散值，回归问题的输出是连续值。虽然Softmax Regressions有个回归，但它是解决分类问题的，而且是多分类问题。对应的，二分类问题有<strong>Logistic Regression</strong>方法。</p>
<p>给定一个输入，比如一张动物图片，经过模型的推理，它告诉我们这张图片有20%的概率是猪，20%的概率是猫，60%的概率是马。Softmax回归就是干这件事儿的。它先算出根据图片中的哪些东西推断出这张图片是猪、是猫、是马（三个类别的<strong>evidence</strong>）。比如，图片中有个尖尖的耳朵，那 evidence(猫) 和 evidence(马)就比evidence(猪)要大，但图片中还有长腿，那evidence(马)就比evidence(猫)和evidence(猪)要大许多了。就这样找好多特征，算出每个特征对应的每个类别的evidence，最后总和起来就得到了这张图片可能是某个类别的evidence。（对于每个类别再加上一定的偏差。）之后softmax回归可以把这些evidence“映射”到每个类别的概率上。</p>
<p>具体到这个例子。图片有10个类别。每个类别i的evidence为：$\text{evidence}_i = \sum_j W x_j + b_i$ </p>
<p>（其中W为 $W_{i,~j}$）。i 是0-9；j 是0-783（或者是1-784）共784个；W一共有10 X 784个，每个W代表每一个像素是某个类别的权重是多少；b是每个类别的偏置。<br>softmax回归的功能就是将这些evidence（共10个）映射到10个概率上。即：$y = \text{softmax}(\text{evidence})$ 这里的y其实是一个数组，其中的每个值代表图片是某个类别的概率。</p>
<h3 id="3-设计模型、训练模型、评估模型"><a href="#3-设计模型、训练模型、评估模型" class="headerlink" title="3. 设计模型、训练模型、评估模型"></a>3. 设计模型、训练模型、评估模型</h3><p>接下来在代码中实现这个softmax回归。</p>
<p>首先设置输入。图像数据x，及其对应的标签y_。使用placeholder。<br>这里的[None, 784]是x这个tensor的形状shape。None表示任意数字。这个tensor是一个2-D（2维）的tensor，秩也为2。但要注意，其中的数据是784维的，即784个数字。2-D的tensor可以当成一个矩阵来看待。每一行代表一个数据，每一行中的每一个数代表一个数据中的一个维度。MNIST中一共有55000个图片，要让计算机一下处理这么多数据很慢。所以可以批量次的“喂给”计算机一部分数据。这里的None就是代表，等会儿“送”None个数据进来。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div></pre></td></tr></table></figure></p>
<p>之后搭建softmax模型。根据上边介绍的公式，需要参数W和b。<br>参数在训练过程中是不停的变的，用tensorflow中的变量Variable。<br>W的shape是[784, 10]，是2-D的tensor，秩为2。相当于一个矩阵。其实它可以是784 X 10也可以是10 X 784的。前者每一行代表图中一个像素是某个类的权重，一行10个数，对应10个类别；后者则是每一列代表图中一个像素是某个类的权重，一列10个数，对应10个类别。<br>b的shape是[10]，是1-D的tensor，秩为1。相当于一个向量，共10个数，即每个类别的偏置。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</div><div class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</div></pre></td></tr></table></figure></p>
<p>之后就是利用数据x，参数W和b和及softmax回归产生每个类别的概率了（tensorflow的厉害之处……）。<br>matmul应该是matrix multiplication的简写，矩阵乘法，x X W 是 None x 784 X 784 x 10，得到的是None X 10的矩阵，再加上b（1 X 10），得到的就是None个图片数据是某个类别的evidence。<br>tensorflow用tf.nn.softmax就可以把evidence映射到每个类别的概率上，而且这里的y的shape是[None, 10]，即None X 10的矩阵，表示None个数据是每个类别的概率。（等等……，现在这个y中存的是概率呢还是进一步被tensorflow转换成“one-hot”数据了呢（这个应该很简单，每一行找出最大值，设为1，其余位置设为0。）？从后边的代码看，应该是前者。）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</div></pre></td></tr></table></figure></p>
<p>现在我们有了预测值，和<a href="http://htxf.github.io/2017/02/23/TensorFlow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/" target="_blank" rel="external">上一篇</a>一样，我们需要算出当前预测的loss（cost）。上一篇中用的是预测值和实际值的差值的平方和。这一篇利用<strong>cross-entropy 交叉熵</strong>。公式是$H_{y’}(y) = -\sum_i y’_i \log(y_i)$ y 是预测值，y’ 是实际值，i 代表每个类别。根据公式写代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * log(y)), reduction_indices=[<span class="number">1</span>])</div></pre></td></tr></table></figure></p>
<p>代码中（y_ * log(y)）的shape，是[None, 10]，reduction_indices=[1]表示做reduce_sum加法时作用在第2维的tensor上。所以（-tf.reduce_sum(y_ * log(y)), reduction_indices=[1]）的shape就成了[None, 1]。最后算出这None个交叉熵的平均值，即用tf.reduce_mean。我们的目的还是为了把模型预测的结果的损失降到最小。即，使得cross_entropy最小。这一篇仍然使用<strong>梯度下降</strong>优化器。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</div></pre></td></tr></table></figure></p>
<p>最后就是把数据“喂给”建好的模型，不断的训练模型，训练出使得损失最小的W和b。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">// 初始化参数变量</div><div class="line">init = tf.global_variables_initializer()</div><div class="line">sess = tf.InteractiveSession()</div><div class="line">sess.run(init)</div><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">    // 每次随机取<span class="number">100</span>个数据来feed给模型进行训练</div><div class="line">    // <span class="number">100</span>就代替了上边的<span class="keyword">None</span></div><div class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</div></pre></td></tr></table></figure></p>
<p>最后的最后，对模型进行评估。这里只是算了下模型在测试数据上预测的准确性。<br>tf.argmax能返回一个tensor某个维度上最大值的位置。y_ 是实际数据，由0、1组成，而且其shape是[None, 10]，第2维上，最大值是1，它的位置（0~9）就刚好是该数据的类别。y 是预测数据，由0到1的小数组成，其shape也是[None, 10]，第2维上，最大值的索引也就代表了该数据被预测为了该索引值。（有个问题！！！要是预测到某两个类别的可能性一样大呢？又刚好是最大的？）tf.equal返回一个布尔值的列表，比较的两者相同就为true。<br>tf.argmax(y, 1)和tf.argmax(y_, 1)的shape是[None, 1]，correct_prediction的shape是[None, 1]，或者是[None]？<br>tf.cast能将布尔值true、false转化为1、0。<br>然后对None个0、1求平均reduce_mean，就刚好是预测的准确率。如：[true false false true]，预测的准确率是50%，对[1 0 0 1]求平均刚好时0.5。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">// 预测的正确与否</div><div class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</div><div class="line">// 预测的准确性</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure></p>
<p>给这块儿的y、y_中的x“喂”测试集上的数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</div></pre></td></tr></table></figure></p>
<h3 id="4-tensorflow中的tensor"><a href="#4-tensorflow中的tensor" class="headerlink" title="4. tensorflow中的tensor"></a>4. tensorflow中的tensor</h3><p>TensorFlow中数据都存在tensor中。上一篇文章没有搞明白tensor的shape、rank。后来又查资料、想了很多。做如下的测试。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">x0 = tf.Variable(tf.random_uniform([]))</div><div class="line">x1 = tf.Variable(tf.random_uniform([<span class="number">3</span>]))</div><div class="line">x2 = tf.Variable(tf.random_uniform([<span class="number">2</span>, <span class="number">3</span>]))</div><div class="line">x3 = tf.Variable(tf.random_uniform([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))</div><div class="line">x4 = tf.Variable(tf.ones([<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))</div><div class="line"></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line"></div><div class="line">sess.run(init)</div><div class="line"></div><div class="line">print(sess.run(x0))</div><div class="line">print(sess.run(x1))</div><div class="line">print(sess.run(x2))</div><div class="line">print(sess.run(x3))</div><div class="line">print(sess.run(x4))</div><div class="line"></div><div class="line">print(sess.run(x1[<span class="number">1</span>]))</div><div class="line">print(sess.run(x2[<span class="number">1</span>, <span class="number">2</span>]))</div><div class="line">print(sess.run(x3[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</div><div class="line">print(sess.run(x4[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</div></pre></td></tr></table></figure></p>
<p>对得到的数据的rank、shape、dimension做如下整理。</p>
<p><img src="..\TensorFLow学习笔记（二）\tensor.jpg" width="100%" height="100%"></p>
<h3 id="5-底层知识"><a href="#5-底层知识" class="headerlink" title="5. 底层知识"></a>5. 底层知识</h3><p>其实本篇所训练出的模型，核心在于<strong>Softmax Regression</strong>、<strong>cross-entropy</strong>以及<strong>gradient decent</strong>。现在只是简单的知道了它们是干什么的。虽然也看过它们的原理、公式的推导过程，但都没有记住……哦多给？？？</p>

      
    </div>
    <footer class="article-footer">
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow学习/">tensorflow学习</a></li></ul>


    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/02/27/JS高程（五）/" id="article-nav-older" class="article-nav-link-wrap">ALI</a>
  
</nav>


  
</article>


</div>
</section>
      </div>
    </div>
  </div>
</body>
</html>
